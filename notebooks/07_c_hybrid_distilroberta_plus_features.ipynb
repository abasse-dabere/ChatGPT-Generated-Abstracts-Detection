{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedb306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80ed729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(name):\n",
    "    name = name.lower()\n",
    "    if name == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    elif name == \"gelu\":\n",
    "        return nn.GELU()\n",
    "    elif name == \"leakyrelu\":\n",
    "        return nn.LeakyReLU(negative_slope=0.1)\n",
    "    elif name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    elif name == \"elu\":\n",
    "        return nn.ELU()\n",
    "    else:\n",
    "        raise ValueError(f\"Activation inconnueâ€¯: {name}\")\n",
    "\n",
    "class DistilRoBERTaWithFeatures(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model_name: str = \"distilroberta-base\",\n",
    "        num_features: int = 38,\n",
    "        hidden_sizes: list[int] = [256], # [32]\n",
    "        activations: list[str] = [\"leakyrelu\"], # [\"leakyrelu\"]\n",
    "        dropout_rates: list[float] = [0.2], # [0.2]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "        self.backbone_dropout = nn.Dropout(0.1)\n",
    "\n",
    "        dims = [self.text_model.config.hidden_size + num_features] + hidden_sizes + [1]\n",
    "        layers: list[nn.Module] = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            in_dim, out_dim = dims[i], dims[i + 1]\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if i < len(hidden_sizes):\n",
    "                layers.append(get_activation(activations[i]))\n",
    "                layers.append(nn.Dropout(dropout_rates[i]))\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, features):\n",
    "        out = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_emb = out.last_hidden_state[:, 0, :]   # [CLS]\n",
    "        cls_emb = self.backbone_dropout(cls_emb)\n",
    "\n",
    "        x = torch.cat([cls_emb, features], dim=-1)\n",
    "        logits = self.classifier(x) # [batch_size]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9acad9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"../scripts/\")\n",
    "# from hybrid_distilroberta_model import DistilRoBERTaWithFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e4b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data\n",
    "data_path = '../data/with_features/'\n",
    "train_data = pd.read_csv(f'{data_path}train.csv')\n",
    "test_data = pd.read_csv(f'{data_path}test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94087b7",
   "metadata": {},
   "source": [
    "## `Human vs Mix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2720bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['num_characters', 'word_count', 'sentence_count',\n",
    "                'mean_sentence_length', 'burstiness', 'stop_words_ratio',\n",
    "                'vocabulary_size', 'ttr', 'tfidf_method', 'tfidf_approach',\n",
    "                'tfidf_proposed', 'tfidf_paper', 'tfidf_study', 'tfidf_analysis',\n",
    "                'tfidf_using', 'tfidf_application', 'tfidf_potential',\n",
    "                'tfidf_performance', 'tfidf_network', 'tfidf_algorithm',\n",
    "                'tfidf_feature', 'tfidf_learning', 'tfidf_data', 'tfidf_model',\n",
    "                'tfidf_control', 'tfidf_information', 'tfidf_accuracy',\n",
    "                'tfidf_technique', 'flesch', 'noun_prop', 'det_prop', 'adj_prop',\n",
    "                'aux_prop', 'pron_prop', 'adv_prop', 'punct_prop', 'adp_prop', 'ppl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0091a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7222, 40)\n",
      "Test shape: (1806, 40)\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "# Keep only human and mix labels\n",
    "Z_train = train_data[train_data['label'].isin(['human', 'mix'])].copy()\n",
    "Z_test = test_data[test_data['label'].isin(['human', 'mix'])].copy()\n",
    "\n",
    "Z_train['label'] = Z_train['label'].map({'human': 0, 'mix': 1})\n",
    "Z_test['label'] = Z_test['label'].map({'human': 0, 'mix': 1})\n",
    "\n",
    "# Split the data into human and mix\n",
    "train_h = Z_train[Z_train['label'] == 0]\n",
    "train_f = Z_train[Z_train['label'] == 1]\n",
    "test_h  = Z_test[Z_test['label']  == 0]\n",
    "test_f  = Z_test[Z_test['label']  == 1]\n",
    "\n",
    "# Number of samples in the minority class\n",
    "n_train = min(len(train_h), len(train_f))\n",
    "n_test  = min(len(test_h),  len(test_f))\n",
    "\n",
    "# Undersample the majority class\n",
    "train_h_down = train_h.sample(n_train, replace=False, random_state=42)\n",
    "train_f_down = train_f.sample(n_train, replace=False, random_state=42)\n",
    "test_h_down  = test_h.sample(n_test,  replace=False, random_state=42)\n",
    "test_f_down  = test_f.sample(n_test,  replace=False, random_state=42)\n",
    "\n",
    "# Concatenate the undersampled data\n",
    "Z_train = pd.concat([train_h_down, train_f_down], axis=0)\n",
    "Z_test  = pd.concat([test_h_down, test_f_down], axis=0)\n",
    "\n",
    "Z_train = Z_train[['abstract'] + feature_cols + ['label']].reset_index(drop=True)\n",
    "Z_test = Z_test[['abstract'] + feature_cols + ['label']].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train shape: {Z_train.shape}\")\n",
    "print(f\"Test shape: {Z_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a8e307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train human: 3611\n",
      "Train mix: 3611\n",
      "Test human: 903\n",
      "Test mix: 903\n"
     ]
    }
   ],
   "source": [
    "# number of data per class\n",
    "print(f\"Train human: {len(Z_train[Z_train['label'] == 0])}\")\n",
    "print(f\"Train mix: {len(Z_train[Z_train['label'] == 1])}\")\n",
    "print(f\"Test human: {len(Z_test[Z_test['label'] == 0])}\")\n",
    "print(f\"Test mix: {len(Z_test[Z_test['label'] == 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4f215c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_feat = scaler.fit_transform(Z_train[feature_cols])\n",
    "X_test_feat = scaler.transform(Z_test[feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60fce1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize abstracts\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "train_enc = tokenizer(Z_train[\"abstract\"].tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_enc = tokenizer(Z_test[\"abstract\"].tolist(), truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c12f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class HybridDataset(Dataset):\n",
    "    def __init__(self, encodings, features, labels):\n",
    "        self.encodings = encodings\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'features': self.features[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e778bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and dataloaders\n",
    "train_dataset = HybridDataset(train_enc, X_train_feat, Z_train[\"label\"])\n",
    "test_dataset = HybridDataset(test_enc, X_test_feat, Z_test[\"label\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e83e1c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2021/abasse.dabere/Desktop/ChatGPT-Generated-Abstracts-Detection/.venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = DistilRoBERTaWithFeatures(num_features=len(feature_cols)).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d22edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "best_auc = 0.0\n",
    "best_model_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f30fe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [01:39<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.6767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:07<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Test Accuracy: 0.6866 | Test AUC: 0.7497\n",
      "--> New best model saved (AUC = 0.7497)\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [01:41<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.5837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:07<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Test Accuracy: 0.6949 | Test AUC: 0.7990\n",
      "--> New best model saved (AUC = 0.7990)\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [01:41<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.4866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:07<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Test Accuracy: 0.7375 | Test AUC: 0.8238\n",
      "--> New best model saved (AUC = 0.8238)\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [01:41<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training Loss: 0.3773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:07<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Test Accuracy: 0.7331 | Test AUC: 0.8275\n",
      "--> New best model saved (AUC = 0.8275)\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [01:41<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training Loss: 0.2892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:07<00:00,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Test Accuracy: 0.6866 | Test AUC: 0.8117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        features = batch[\"features\"].to(device)\n",
    "        labels = batch[\"labels\"].unsqueeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask, features)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    #Evaluation\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            features = batch[\"features\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask, features)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "\n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_auc = roc_auc_score(all_targets, all_probs)\n",
    "    epoch_acc = accuracy_score(all_targets, all_preds)\n",
    "    print(f\"Epoch {epoch+1} - Test Accuracy: {epoch_acc:.4f} | Test AUC: {epoch_auc:.4f}\")\n",
    "\n",
    "    # Save the best model based on AUC\n",
    "    if epoch_auc > best_auc:\n",
    "        best_auc = epoch_auc\n",
    "        best_model_state = model.state_dict()  # ou torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(f\"--> New best model saved (AUC = {best_auc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66a33717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished. Best AUC obtained on test: 0.8275\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(best_model_state)\n",
    "print(f\"Training finished. Best AUC obtained on test: {best_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "842c0638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/human_vs_mix/tokenizer_hybrid/tokenizer_config.json',\n",
       " '../models/human_vs_mix/tokenizer_hybrid/special_tokens_map.json',\n",
       " '../models/human_vs_mix/tokenizer_hybrid/vocab.json',\n",
       " '../models/human_vs_mix/tokenizer_hybrid/merges.txt',\n",
       " '../models/human_vs_mix/tokenizer_hybrid/added_tokens.json',\n",
       " '../models/human_vs_mix/tokenizer_hybrid/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"../models/human_vs_mix/hybrid_model.pth\")\n",
    "# Save scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, '../models/human_vs_mix/scaler_hybrid.pkl')\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('../models/human_vs_mix/tokenizer_hybrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71442560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
